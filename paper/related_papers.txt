================================================================================
RELATED PAPERS FOR MEDSAFETYBENCH++ SURVEY
================================================================================
20 Key Papers (Prioritizing 2024-2025, Top Venues: NeurIPS, ICML, ICLR)
================================================================================

FOUNDATIONAL MEDICAL AI SAFETY (2024):
1. MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models (2024)
   arXiv 2403.03744: https://arxiv.org/abs/2403.03744

2. MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records (ACL 2024)
   arXiv 2308.14089: https://arxiv.org/abs/2308.14089

3. Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine (2024)
   arXiv 2311.16452: https://arxiv.org/abs/2311.16452

JAILBREAKING & PROMPT INJECTION (2024-2025):
4. Jailbreaking Black Box Large Language Models in Twenty Queries (2024)
   arXiv 2310.08419: https://arxiv.org/abs/2310.08419

5. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models (ICLR 2024)
   arXiv 2310.04451: https://arxiv.org/abs/2310.04451

6. Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation (2024)
   arXiv 2311.03348: https://arxiv.org/abs/2311.03348

7. How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety (2024)
   arXiv 2401.06373: https://arxiv.org/abs/2401.06373

PRIVACY ATTACKS ON LLMs (2024):
8. Scalable Extraction of Training Data from (Production) Language Models (2024)
   arXiv 2311.17035: https://arxiv.org/abs/2311.17035

9. Privacy Backdoors: Stealing Data with Corrupted Pretrained Models (2024)
   arXiv 2404.00473: https://arxiv.org/abs/2404.00473

10. Stronger Privacy Guarantees for Synthetic Data (2024)
    arXiv 2401.12391: https://arxiv.org/abs/2401.12391

MULTIMODAL ADVERSARIAL ATTACKS (2024):
11. Visual Adversarial Examples Jailbreak Aligned Large Language Models (2024)
    arXiv 2306.13213: https://arxiv.org/abs/2306.13213

12. Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models (ICLR 2024)
    arXiv 2307.14539: https://arxiv.org/abs/2307.14539

13. On the Adversarial Robustness of Multi-Modal Foundation Models (2024)
    arXiv 2308.10741: https://arxiv.org/abs/2308.10741

14. SneakyPrompt: Jailbreaking Text-to-image Generative Models (2024)
    arXiv 2305.12082: https://arxiv.org/abs/2305.12082

BIAS & FAIRNESS IN MEDICAL AI (2024):
15. Evaluating and Mitigating Discrimination in Language Model Decisions (2024)
    arXiv 2312.03689: https://arxiv.org/abs/2312.03689

16. BiasAsker: Measuring the Bias in Conversational AI System (2024)
    arXiv 2305.12434: https://arxiv.org/abs/2305.12434

17. Fairness in Large Language Models: A Taxonomic Survey (2024)
    arXiv 2308.10149: https://arxiv.org/abs/2308.10149

DEFENSE MECHANISMS (2024):
18. Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM (2024)
    arXiv 2309.14348: https://arxiv.org/abs/2309.14348

19. Baseline Defenses for Adversarial Attacks Against Aligned Language Models (2024)
    arXiv 2309.00614: https://arxiv.org/abs/2309.00614

20. SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks (2024)
    arXiv 2310.03684: https://arxiv.org/abs/2310.03684


================================================================================
FOUNDATIONAL CLASSICS (Keep These - Too Important):
================================================================================

21. Constitutional AI: Harmlessness from AI Feedback (Anthropic, 2022)
    arXiv 2212.08073: https://arxiv.org/abs/2212.08073
    [FOUNDATIONAL for defense mechanisms]

22. Adversarial Examples Are Not Bugs, They Are Features (NeurIPS 2019)
    arXiv 1905.02175: https://arxiv.org/abs/1905.02175
    [FOUNDATIONAL adversarial ML theory]

23. Extracting Training Data from Large Language Models (USENIX Security 2021)
    arXiv 2012.07805: https://arxiv.org/abs/2012.07805
    [FOUNDATIONAL privacy attack - first major LLM memorization work]


================================================================================
ADDITIONAL 2024-2025 ALTERNATIVES:
================================================================================

MEDICAL AI SPECIFIC (2024):
- HealthAgent: Generalist LLM Agents for Healthcare (2024)
  https://arxiv.org/abs/2402.12886

- ClinicalBench: Can LLMs Beat Traditional ML Models on Clinical Prediction? (2024)
  https://arxiv.org/abs/2404.02897

- Med-Flamingo: A Multimodal Medical Few-shot Learner (2023)
  https://arxiv.org/abs/2307.15189

JAILBREAKING (2024-2025):
- GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher (2024)
  https://arxiv.org/abs/2308.06463

- Tree of Attacks: Jailbreaking Black-Box LLMs Automatically (2024)
  https://arxiv.org/abs/2312.02119

- MasterKey: Automated Jailbreak Across Multiple LLM Chatbots (NDSS 2024)
  https://arxiv.org/abs/2307.08715

MULTIMODAL SAFETY (2024):
- FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts (2024)
  https://arxiv.org/abs/2311.05608

- How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for VLMs (2024)
  https://arxiv.org/abs/2311.16101

ROBUSTNESS & DEFENSE (2024):
- SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding (ACL 2024)
  https://arxiv.org/abs/2402.08983

- Self-Guard: Empower the LLM to Safeguard Itself (2024)
  https://arxiv.org/abs/2310.15851


================================================================================
SEARCH STRATEGY FOR 2024-2025 PAPERS:
================================================================================

1. arXiv Recent Submissions (2024-2025):
   - cs.CR (Security): https://arxiv.org/list/cs.CR/recent
   - cs.AI: https://arxiv.org/list/cs.AI/recent  
   - cs.CL (NLP): https://arxiv.org/list/cs.CL/recent
   - cs.LG: https://arxiv.org/list/cs.LG/recent

2. Conference Proceedings (2024-2025):
   - NeurIPS 2024: https://neurips.cc/virtual/2024/papers.html
   - ICML 2024: https://icml.cc/virtual/2024/papers.html
   - ICLR 2024: https://iclr.cc/virtual/2024/papers.html
   - ACL 2024: https://2024.aclweb.org/program/accepted_main_conference/

3. Google Scholar with filters:
   - "jailbreak LLM" since:2024
   - "medical AI adversarial" since:2024
   - "multimodal safety" since:2024
   - "LLM privacy attack" since:2024

4. Papers with Code (trending):
   https://paperswithcode.com/
   Filter: Safety, Adversarial, Medical


================================================================================
CITATION PRIORITIES:
================================================================================

MUST CITE (Core to your contributions):
✓ MedSafetyBench (2024) - Your foundation
✓ Recent jailbreaking papers (2024) - Attack methods
✓ Multimodal adversarial (2024) - Your multimodal contribution
✓ Privacy attacks (2024) - Medical privacy concerns
✓ Constitutional AI (2022) - Defense mechanism (too foundational to skip)

GOOD TO CITE:
✓ Medical AI bias papers (2024)
✓ Defense mechanisms (2024)
✓ LLM safety evaluation frameworks

OPTIONAL (if space):
✓ Foundational adversarial ML (pre-2024) - only the most cited
