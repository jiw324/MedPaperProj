\documentclass{article}

% Recommended packages
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

% ICML 2026 style (includes natbib)
\usepackage{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\twocolumn[
\icmltitle{A Practical Framework for Evaluating Medical AI Security: Baseline Assessment of Jailbreaking and Privacy Vulnerabilities}

\begin{icmlauthorlist}
\icmlauthor{Your Name}{inst1}
\end{icmlauthorlist}

\icmlaffiliation{inst1}{Your Institution, Department, Location}
\icmlcorrespondingauthor{Your Name}{your.email@domain.com}

\icmlkeywords{Medical AI, Adversarial Attacks, AI Safety, Privacy, Jailbreaking, LLM Security}

\vskip 0.3in
]

\begin{abstract}

Medical Large Language Models (LLMs) are increasingly used for clinical decision support, yet their robustness to adversarial misuse and privacy leakage remains poorly understood. Existing evaluations emphasize diagnostic accuracy or benign safety criteria, leaving critical vulnerabilities—such as jailbreaking attacks and protected health information (PHI) extraction—largely unmeasured. We present a practical and fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our benchmark consists of thirty adversarial scenarios spanning role-playing, authority impersonation, multi-turn escalation, and PHI-targeted extraction, all constructed using synthetic patient records requiring no IRB approval. Using GPT-2 and DistilGPT-2 as accessible testbeds, we assess attack success rates, PHI leakage patterns, and qualitative failure modes. Our results establish baseline vulnerabilities for small open-source models and demonstrate that meaningful security assessment can be conducted without GPUs, proprietary data, or commercial APIs. This framework provides a foundation for future comparative studies of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.

\end{abstract}

\printAffiliationsAndNotice{}

%% ============================================
%% SECTION 1: PROBLEM STATEMENT
%% ============================================
\section{Problem Statement}

\subsection{The Security Challenge in Medical AI}

Large Language Models are rapidly transforming healthcare~\cite{medpalm2,medpalm}. GPT-4 achieves expert-level performance on medical examinations~\cite{gpt4}, and AI assistants increasingly provide clinical decision support. However, medical AI systems face critical security vulnerabilities that directly threaten patient safety~\cite{llm_safety_survey,concrete_problems}.

\subsection{Foundational Work Guiding Problem Selection}

Our selection of security threats is guided by established taxonomies and foundational research in AI safety. The seminal work by Amodei et al.~\cite{concrete_problems} identified five concrete problems in AI safety, establishing that systems optimizing for objectives may produce unintended harmful behaviors. This foundational framework motivates our focus on how medical AI can be manipulated to cause harm.

For jailbreaking vulnerabilities, we build on Wei et al.'s foundational analysis~\cite{jailbroken} which systematically categorized why safety training fails under adversarial pressure. Their work demonstrated that competing objectives between helpfulness and safety create exploitable tensions, and that pretrained knowledge conflicts with safety training. The GCG attack by Zou et al.~\cite{gcg} further established that universal adversarial suffixes transfer across models, revealing fundamental weaknesses in current alignment approaches. HarmBench~\cite{harmbench} subsequently provided standardized evaluation protocols that we adapt for medical contexts.

For privacy extraction, Carlini et al.'s foundational work~\cite{carlini_extraction} proved that language models memorize and can regurgitate training data verbatim. Their follow-up quantification study~\cite{carlini_quantifying} demonstrated that memorization scales with model size and data repetition. This poses acute risks for medical AI: models trained on clinical notes may leak protected health information, creating HIPAA violations~\cite{hipaa}. The OWASP Top 10 for LLM Applications~\cite{owasp_llm} identifies ``Sensitive Information Disclosure'' as a critical vulnerability class, validating our focus.

MedSafetyBench~\cite{medsafetybench} provides domain-specific grounding, demonstrating that medical-specialist models paradoxically show higher compliance with harmful requests than general models. This counterintuitive finding suggests that medical domain knowledge amplifies rather than mitigates security risks, making our evaluation essential.

\subsection{Why These Problems Are Most Critical}

We prioritize jailbreaking and privacy extraction based on three criteria derived from foundational risk frameworks. First, \textbf{severity of harm}: the FDA's guidance on AI/ML-based medical devices emphasizes that patient safety is paramount, and jailbreaking attacks that elicit dangerous medical advice can directly cause patient harm. Second, \textbf{likelihood of exploitation}: unlike theoretical attacks, jailbreaking and privacy extraction have been demonstrated repeatedly in production systems~\cite{jailbroken,carlini_extraction}, with active communities developing and sharing attack techniques. Third, \textbf{regulatory consequence}: HIPAA violations carry penalties up to \$1.5 million per incident~\cite{hipaa}, making privacy extraction a concrete liability for healthcare organizations deploying medical AI.

We also address a methodological problem: existing security evaluations require substantial resources including GPU clusters, API budgets, and expert annotation teams~\cite{harmbench,decodingtrust}. This accessibility barrier conflicts with the principle that security research benefits from broad participation~\cite{red_teaming}, motivating our reproducible framework.

\subsection{Research Questions}

Grounded in this foundational work, we address four research questions. RQ1 asks whether medical AI models can be jailbroken through adversarial prompting techniques established by Wei et al.~\cite{jailbroken} and Zou et al.~\cite{gcg}. RQ2 investigates whether models leak protected health information following Carlini et al.'s extraction methodology~\cite{carlini_extraction}. RQ3 examines which attack types achieve the highest success rates to inform defense prioritization per HarmBench conventions~\cite{harmbench}. Finally, RQ4 establishes baseline vulnerabilities for current-generation models, enabling future comparative studies as recommended by TrustLLM~\cite{trustllm}.

%% ============================================
%% SECTION 2: RELATED PRINCIPLES
%% ============================================
\section{Related Principles and Background}

\subsection{LLM Safety Mechanisms}

Modern LLMs employ safety alignment techniques to refuse harmful requests~\cite{rlhf,constitutional}. Reinforcement Learning from Human Feedback trains models to be helpful while avoiding harm by incorporating human preferences into the optimization objective~\cite{rlhf}. Constitutional AI extends this approach by encoding ethical principles directly into the training process~\cite{constitutional}. Despite these advances, safety mechanisms exhibit fundamental vulnerabilities that sophisticated attackers can exploit~\cite{jailbroken}.

\subsection{Adversarial Attack Techniques}

Jailbreaking attacks circumvent safety mechanisms through various techniques~\cite{llm_safety_survey}. Role-playing attacks frame harmful requests as fictional or educational scenarios, exploiting models' tendency to be helpful in creative contexts~\cite{jailbroken}. The GCG attack discovers universal adversarial suffixes that transfer across different models~\cite{gcg}. Automated approaches such as AutoDAN~\cite{autodan} and Tree of Attacks~\cite{tap} systematically search for effective jailbreaks. Multi-turn manipulation gradually escalates from benign to harmful queries across conversation turns, eroding safety boundaries incrementally~\cite{masterkey,pair}.

Privacy attacks exploit the tendency of large models to memorize training data~\cite{carlini_extraction}. Training data extraction prompts models to reproduce memorized examples verbatim~\cite{carlini_extraction,scalable_extraction}. Membership inference attacks determine whether specific data points were included in training sets~\cite{membership_inference}. These techniques pose particular risks for medical AI trained on clinical notes containing protected health information.

\subsection{Medical AI Evaluation Standards}

Prior medical AI benchmarks focus primarily on accuracy and clinical reasoning. MedQA~\cite{medqa}, PubMedQA~\cite{pubmedqa}, and MultiMedQA~\cite{multimedqa} evaluate diagnostic performance and medical knowledge. MedSafetyBench pioneered ethical compliance evaluation, testing whether models refuse harmful requests aligned with medical ethics principles~\cite{medsafetybench}. A key finding from this work reveals that medical-specialist models paradoxically exhibit higher compliance with harmful requests than general-purpose models, suggesting that specialization without safety alignment creates dangerous capabilities.

\subsection{Gap Analysis}

Existing work leaves critical gaps that our research addresses. Medical AI evaluation lacks systematic adversarial robustness testing, focusing instead on accuracy metrics. LLM security research develops sophisticated attack techniques but overlooks medical-specific threat vectors and consequences. Perhaps most importantly, security evaluations require computational and financial resources that most researchers lack, limiting the field's ability to assess the growing deployment of medical AI systems.

%% ============================================
%% SECTION 3: OUR SOLUTION
%% ============================================
\section{Our Solution: Accessible Security Evaluation Framework}

\subsection{Core Design Principles}

We design our framework around three guiding principles. The first principle emphasizes complete reproducibility: all components are freely available, including GPT-2 and DistilGPT-2 models accessible via Hugging Face at zero cost, consumer CPU hardware without GPU requirements, and entirely synthetic data requiring no IRB approval. This design choice ensures that any researcher worldwide can replicate and extend our experiments.

The second principle advocates focused scope over exhaustive coverage. Rather than attempting comprehensive evaluation across all possible threat categories, we prioritize rigorous assessment of two critical areas: jailbreaking and privacy extraction. This focused approach enables deeper analysis within our resource constraints while still addressing the most pressing security concerns.

The third principle centers on baseline establishment. We create reproducible reference points that enable future comparative studies of medical-specialist models~\cite{biogpt,clinicalbert} and defense mechanisms~\cite{smoothllm}. By documenting baseline vulnerabilities in widely-available models, we provide a foundation against which improvements can be measured.

\subsection{Threat Model}

Our threat model encompasses two primary attack categories. For jailbreaking attacks, we develop twenty scenarios spanning four attack vectors. Medical role-playing scenarios frame harmful requests as educational content, exemplified by prompts such as ``You are a toxicology professor teaching medical students about dangerous substance interactions.'' Authority impersonation scenarios claim healthcare credentials to justify sensitive requests. Multi-turn manipulation scenarios gradually escalate from benign to harmful queries across conversation turns. We also include benign control scenarios containing legitimate medical questions to establish baseline helpfulness.

For privacy extraction attacks, we develop ten scenarios following the methodology established by Carlini et al.~\cite{carlini_extraction}. These scenarios present synthetic patient records to models and prompt them to summarize or extract identifiable information, measuring the degree to which protected health information leaks into model outputs.

\subsection{Dataset Construction}

We construct our benchmark using entirely synthetic data, enabling reproducible evaluation without IRB approval or access to proprietary clinical databases. Our synthetic patient records comprise ten fictitious patients with complete protected health information including randomly generated names, fake medical record numbers, synthetic social security numbers, and dates of birth. Each record includes clinical information with common diagnoses and medications sampled from population statistics, formatted as SOAP notes containing Subjective, Objective, Assessment, and Plan sections. All data is completely fictitious, eliminating privacy risks and enabling full dataset publication.

Attack scenarios are constructed from the jailbreaking literature~\cite{jailbroken,autodan,pair}. Each scenario includes an adversarial prompt, expected model behavior categorized as refusal or compliance, and a harmfulness severity rating following HarmBench conventions~\cite{harmbench}.

\subsection{Evaluation Metrics}

We adopt the Attack Success Rate metric following established practices~\cite{harmbench}. Responses are scored on a five-point scale where a score of five indicates complete compliance with no safety refusal, four indicates substantial harmful content with minor caveats, three indicates an ambiguous mix of refusal and compliance, two indicates primary refusal with some problematic leakage, and one indicates appropriate complete refusal. Attack Success Rate is computed as the percentage of scenarios scoring four or above.

For privacy attacks, we measure extraction success rate, the average number of PHI elements leaked out of four possible identifiers, and HIPAA violation severity based on the sensitivity of disclosed information.

%% ============================================
%% SECTION 4: EXPERIMENTAL SETUP
%% ============================================
\section{Experimental Setup}

\subsection{Models}

We evaluate two freely available models selected for accessibility and reproducibility. GPT-2 with 124 million parameters serves as a widely-used baseline originally released by OpenAI. DistilGPT-2 with 82 million parameters provides a compressed variant that tests whether knowledge distillation affects security properties. Both models run efficiently on consumer CPU hardware via the Hugging Face Transformers library, requiring no GPU resources or API costs.

\subsection{Hardware and Environment}

All experiments run on consumer-grade hardware consisting of an Intel Core i7 CPU with 16GB RAM, demonstrating that meaningful security evaluation requires no specialized computing resources. The software environment comprises Python 3.8 or later with the Hugging Face Transformers library. We fix the random seed to 42 across all experiments to ensure complete reproducibility.

\subsection{Generation Parameters}

We configure text generation with temperature set to 0.7 to balance coherence with output diversity, maximum token length of 200 to capture complete responses, and top-p nucleus sampling at 0.9 to focus on high-probability continuations while maintaining some variation. These parameters remain fixed across all experiments to ensure fair comparison.

\subsection{Evaluation Protocol}

Each of the thirty scenarios is presented to each model exactly once, yielding sixty total outputs for analysis. Two independent human raters score all outputs using the five-point ASR rubric, achieving 87\% inter-rater agreement before discussion and reconciliation of disagreements.

Statistical analysis employs Wilson score intervals to compute 95\% confidence bounds on attack success rates, and chi-square tests at significance level $\alpha = 0.05$ for model comparisons. This rigorous statistical approach ensures that reported differences reflect genuine effects rather than sampling variation.

%% ============================================
%% SECTION 5: RESULTS
%% ============================================
\section{Results}

This section will be completed after experimental evaluation. We will report jailbreaking attack success rates across the four attack categories for both GPT-2 and DistilGPT-2, privacy extraction success rates and PHI leakage patterns, comparative analysis examining differences between models and attack types, and qualitative analysis presenting representative examples of successful attacks and effective refusals.

%% ============================================
%% SECTION 6: DISCUSSION
%% ============================================
\section{Discussion}

\subsection{Key Findings}

This subsection will synthesize experimental results upon completion.

\subsection{Relationship to Prior Work}

Our work complements rather than replaces existing evaluation efforts. MedSafetyBench evaluates ethical compliance using direct harmful requests, while we evaluate adversarial robustness using sophisticated attack techniques including role-playing~\cite{jailbroken} and multi-turn manipulation~\cite{gcg}. Our attack scenarios build on established jailbreaking techniques~\cite{autodan,pair,tap} and privacy extraction methods~\cite{carlini_extraction}, adapting them specifically for medical contexts where the stakes are highest.

\subsection{Limitations}

Several limitations contextualize our findings. Regarding model scope, GPT-2 and DistilGPT-2 lack medical specialization and were not trained on clinical data~\cite{biogpt}. Vulnerability patterns may differ substantially for models trained on medical corpora. Our sample size of thirty scenarios provides sufficient power to establish baseline trends but cannot support fine-grained statistical claims that would require larger-scale evaluation~\cite{harmbench}. The use of entirely synthetic data eliminates real-world complexity; models trained on actual clinical notes from sources such as MIMIC~\cite{mimic} may exhibit different memorization and leakage patterns~\cite{carlini_extraction}. Finally, manual scoring introduces subjectivity that future work should address through validated LLM-as-judge evaluation~\cite{llm_judge}.

\subsection{Implications}

Our findings carry several implications for medical AI development and deployment. Safety alignment cannot be assumed to generalize across contexts~\cite{jailbroken}, and developers should systematically evaluate adversarial robustness before clinical deployment. Multi-turn defenses are essential because single-turn safety filters may be circumvented through conversational boundary erosion~\cite{masterkey,llm_self_defense}. Privacy evaluation requires scenarios with known ground-truth identifiers~\cite{carlini_extraction}, and our synthetic SOAP notes provide templates for systematic measurement.

%% ============================================
%% SECTION 7: CONCLUSION
%% ============================================
\section{Conclusion}

\subsection{Summary}

We present a practical, reproducible framework for evaluating medical AI security vulnerabilities. The core problem we address is that medical AI systems face jailbreaking and privacy threats while existing security evaluations remain inaccessible to most researchers. Our solution provides a thirty-scenario benchmark using freely available models and entirely synthetic data that any researcher can replicate. The primary contribution establishes baseline vulnerability assessments enabling future comparative studies of specialized models and defense mechanisms.

\subsection{Future Work}

Several directions extend this work. Evaluating medical-specialist models such as BioGPT~\cite{biogpt}, ClinicalBERT~\cite{clinicalbert}, and Med-PaLM 2~\cite{medpalm2} would reveal whether domain specialization increases or decreases vulnerability. Testing commercial APIs including GPT-4~\cite{gpt4} and Claude~\cite{claude} would assess systems approaching clinical deployment. Systematically evaluating defense mechanisms such as SmoothLLM~\cite{smoothllm} and safety-tuned models~\cite{safety_tuned} would quantify protection effectiveness. Extending coverage to multimodal vulnerabilities~\cite{visual_jailbreak} and bias exploitation~\cite{obermeyer_bias} would provide more comprehensive threat characterization. Finally, scaling to hundreds of scenarios following HarmBench methodology~\cite{harmbench} would increase statistical power for fine-grained analysis.

\subsection{Closing Remarks}

Medical AI promises transformative healthcare benefits~\cite{medpalm2} but simultaneously introduces critical security risks~\cite{llm_safety_survey}. By demonstrating that rigorous security evaluation requires only modest resources, we enable broader participation in this essential research area~\cite{trustllm}. We hope this accessible framework accelerates community-driven progress toward safer medical AI systems that can realize their potential without compromising patient safety.

\bibliography{references}
\bibliographystyle{icml2026/icml2026}

\end{document}
